#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯é€± AI å¿«å ± v2 - æ·±åº¦åˆ†æç‰ˆ
çµåˆ Redditã€Product Huntã€æ©Ÿå™¨ä¹‹å¿ƒã€é‡å­ä½ çš„åŸå§‹è³‡æ–™ï¼Œ
é€é Claude API ç”Ÿæˆå°ˆæ¥­æ·±åº¦åˆ†æï¼Œæ¯é€±ä¸€æ—©ä¸Šè‡ªå‹•ç™¼é€è‡³ Telegramã€‚

å®‰è£ä¾è³´ï¼špip install anthropic
åŸ·è¡Œæ–¹å¼ï¼špython daily_ai_news.py          # æ­£å¼ç™¼é€
          python daily_ai_news.py --test   # åƒ…é è¦½ï¼Œä¸ç™¼é€
"""

import sys
import os
import json
import urllib.request
import urllib.error
import xml.etree.ElementTree as ET
from datetime import datetime
from pathlib import Path

# â”€â”€ æª¢æŸ¥ anthropic å¥—ä»¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import anthropic
except ImportError:
    print("âŒ ç¼ºå°‘ anthropic å¥—ä»¶ï¼Œè«‹å…ˆåŸ·è¡Œï¼špip install anthropic")
    sys.exit(1)

# â”€â”€ å¾ .env è¼‰å…¥ç’°å¢ƒè®Šæ•¸ï¼ˆè‹¥å°šæœªè¨­å®šï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_env():
    env_path = Path(__file__).parent / ".env"
    if not env_path.exists():
        return
    with open(env_path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith("#") and "=" in line:
                k, v = line.split("=", 1)
                os.environ.setdefault(k.strip(), v.strip())

_load_env()

# =====================================================================
# è¨­å®šå€ï¼šå„ªå…ˆè®€å–ç’°å¢ƒè®Šæ•¸ï¼Œå…¶æ¬¡è®€å– .env æª”
# è«‹å°‡å¯¦éš›é‡‘é‘°å¡«å…¥åŒç›®éŒ„çš„ .env æª”ï¼ˆåƒè€ƒ .env.exampleï¼‰
# =====================================================================
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY", "")
BOT_TOKEN         = os.environ.get("TELEGRAM_BOT_TOKEN", "")

# æ”¯æ´å¤šå€‹ Chat IDï¼Œä»¥é€—è™Ÿåˆ†éš”ï¼Œä¾‹å¦‚ï¼š112966076,987654321
_raw_ids  = os.environ.get("TELEGRAM_CHAT_ID", "")
CHAT_IDS  = [cid.strip() for cid in _raw_ids.split(",") if cid.strip()]
CHAT_ID   = CHAT_IDS[0] if CHAT_IDS else ""   # å‘ä¸‹ç›¸å®¹

# claude-haikuï¼šå¿«é€Ÿã€ä½æˆæœ¬ï¼ˆæ¯æ¬¡å ±å‘Šç´„ $0.001ï½0.003 ç¾å…ƒï¼‰
CLAUDE_MODEL = "claude-haiku-4-5-20251001"
TIMEOUT      = 25

# å ±å‘Šå¿«å–ï¼ˆæ¯æ¬¡ç”Ÿæˆå¾Œå­˜æª”ï¼Œ/preview å„ªå…ˆå¾æ­¤è®€å–ï¼‰
DATA_DIR            = Path(os.environ.get("DATA_DIR", Path(__file__).parent / "data"))
REPORT_CACHE_FILE   = DATA_DIR / "report_cache.json"
PREV_TITLES_FILE    = DATA_DIR / "prev_titles.json"   # ä¸ŠæœŸå·²å ±å°æ¨™é¡Œï¼Œç”¨æ–¼å»é‡
# =====================================================================

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/121.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8",
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HTTP å·¥å…·
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def http_get(url, extra=None):
    h = dict(HEADERS)
    if extra:
        h.update(extra)
    req = urllib.request.Request(url, headers=h)
    try:
        with urllib.request.urlopen(req, timeout=TIMEOUT) as r:
            cs = r.headers.get_content_charset() or "utf-8"
            return r.read().decode(cs, errors="replace")
    except Exception as e:
        print(f"  âš ï¸  [{url[:55]}] {e}", file=sys.stderr)
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è³‡æ–™æ”¶é›†ï¼šReddit
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_reddit(top_n=8):
    subs = ["artificial", "MachineLearning", "LocalLLaMA", "ChatGPT", "singularity"]
    posts = []
    for sub in subs:
        raw = http_get(
            f"https://www.reddit.com/r/{sub}/hot.json?limit=5",
            extra={"Accept": "application/json"},
        )
        if not raw:
            continue
        try:
            for c in json.loads(raw)["data"]["children"]:
                p = c["data"]
                if p.get("stickied"):
                    continue
                posts.append({
                    "source":   f"Reddit r/{sub}",
                    "title":    p["title"][:150],
                    "url":      f"https://reddit.com{p['permalink']}",
                    "score":    p.get("score", 0),
                    "comments": p.get("num_comments", 0),
                })
        except Exception as e:
            print(f"  âš ï¸  Reddit [{sub}]: {e}", file=sys.stderr)
    posts.sort(key=lambda x: x["score"], reverse=True)
    return posts[:top_n]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸ŠæœŸæ¨™é¡Œå»é‡ï¼šé¿å…æœ¬é€±å ±å‘Šé‡è¤‡ä¸Šé€±æ–°è
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_prev_titles() -> list[str]:
    """è®€å–ä¸ŠæœŸå·²å ±å°çš„æ¨™é¡Œæ¸…å–®"""
    if not PREV_TITLES_FILE.exists():
        return []
    try:
        with open(PREV_TITLES_FILE, encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []


def save_prev_titles(titles: list[str]):
    """å°‡æœ¬æœŸæ¨™é¡Œå­˜æª”ï¼Œä¾›ä¸‹æœŸå»é‡ä½¿ç”¨"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    with open(PREV_TITLES_FILE, "w", encoding="utf-8") as f:
        json.dump(titles[:80], f, ensure_ascii=False, indent=2)


def _is_duplicate(title: str, prev_titles: list[str], threshold: int = 10) -> bool:
    """åˆ¤æ–·æ¨™é¡Œæ˜¯å¦èˆ‡ä¸ŠæœŸé‡è¤‡ï¼ˆå–å‰ threshold å€‹å­—å…ƒåšæ¨¡ç³Šæ¯”å°ï¼‰"""
    t = title.lower()[:threshold]
    return any(t and t in p.lower() for p in prev_titles)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è³‡æ–™æ”¶é›†ï¼šRSSï¼ˆProduct Hunt / æ©Ÿå™¨ä¹‹å¿ƒ / é‡å­ä½ / å®˜æ–¹ Blog ç­‰ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_rss(url, source_name, max_items=5, ai_filter=False):
    AI_KW = [
        "ai", "llm", "gpt", "chatbot", "machine learning", "agent",
        "artificial intelligence", "automation", "model", "neural",
        "äººå·¥æ™ºèƒ½", "æ©Ÿå™¨å­¸ç¿’", "å¤§æ¨¡å‹", "ç”Ÿæˆå¼", "æ™ºèƒ½",
    ]
    raw = http_get(url)
    if not raw:
        return []
    items = []
    try:
        root = ET.fromstring(raw.replace("xmlns=", "xmlnamespace="))
        channel = root.find("channel")
        if channel is not None:
            src_items = channel.findall("item")
            def get_title(el): return (el.findtext("title") or "").strip()
            def get_link(el):  return (el.findtext("link")  or "").strip()
            def get_desc(el):  return (el.findtext("description") or "").strip()[:250]
        else:
            ns = "http://www.w3.org/2005/Atom"
            src_items = root.findall(f"{{{ns}}}entry")
            def get_title(el): return (el.findtext(f"{{{ns}}}title") or "").strip()
            def get_link(el):
                lel = el.find(f"{{{ns}}}link")
                return lel.get("href", "") if lel is not None else ""
            def get_desc(el):  return (el.findtext(f"{{{ns}}}summary") or "").strip()[:250]

        for item in src_items:
            t = get_title(item)
            l = get_link(item)
            d = get_desc(item)
            if ai_filter and not any(k in (t + d).lower() for k in AI_KW):
                continue
            if t and l:
                items.append({"source": source_name, "title": t[:150],
                              "url": l, "summary": d})
            if len(items) >= max_items:
                break
    except ET.ParseError as e:
        print(f"  âš ï¸  RSS [{source_name}]: {e}", file=sys.stderr)
    return items


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Claude APIï¼šç”Ÿæˆæ·±åº¦åˆ†æå ±å‘Š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_report(raw_data: dict) -> str:
    now   = datetime.now()
    date  = now.strftime("%Y/%m/%d")
    year  = now.year
    month = now.month

    # æ•´ç†åŸå§‹è³‡æ–™æ–‡å­—
    ctx = []

    # æ­ç¾ä¾†æº
    if raw_data.get("reddit"):
        ctx.append("ã€Reddit ç†±é–€è²¼æ–‡ï¼ˆä¾è®šæ•¸æ’åºï¼‰ã€‘")
        for p in raw_data["reddit"]:
            ctx.append(f"- [{p['source']}] {p['title']}  â¬†ï¸{p['score']} ğŸ’¬{p['comments']}  {p['url']}")
    if raw_data.get("openai"):
        ctx.append("\nã€OpenAI Blog æœ€æ–°æ–‡ç« ã€‘")
        for p in raw_data["openai"]:
            ctx.append(f"- {p['title']} | {p['url']}")
    if raw_data.get("anthropic"):
        ctx.append("\nã€Anthropic Blog æœ€æ–°æ–‡ç« ã€‘")
        for p in raw_data["anthropic"]:
            ctx.append(f"- {p['title']} | {p['url']}")
    if raw_data.get("deepmind"):
        ctx.append("\nã€Google DeepMind Blog æœ€æ–°æ–‡ç« ã€‘")
        for p in raw_data["deepmind"]:
            ctx.append(f"- {p['title']} | {p['url']}")
    if raw_data.get("producthunt"):
        ctx.append("\nã€Product Hunt ä»Šæ—¥ AI å·¥å…·ã€‘")
        for p in raw_data["producthunt"]:
            ctx.append(f"- {p['title']} | {p.get('summary','')} | {p['url']}")
    if raw_data.get("techinasia"):
        ctx.append("\nã€Tech in Asia æœ€æ–° AI å ±å°ã€‘")
        for p in raw_data["techinasia"]:
            ctx.append(f"- {p['title']} | {p['url']}")
    if raw_data.get("cna"):
        ctx.append("\nã€CNA Tech æœ€æ–°ç§‘æŠ€æ–°èã€‘")
        for p in raw_data["cna"]:
            ctx.append(f"- {p['title']} | {p['url']}")

    # ä¸­æ–‡ä¾†æº
    if raw_data.get("jiqizhixin"):
        ctx.append("\nã€æ©Ÿå™¨ä¹‹å¿ƒ æœ€æ–°æ–‡ç« ã€‘")
        for p in raw_data["jiqizhixin"]:
            ctx.append(f"- {p['title']} | {p['url']}")
    if raw_data.get("qbitai"):
        ctx.append("\nã€é‡å­ä½ æœ€æ–°æ–‡ç« ã€‘")
        for p in raw_data["qbitai"]:
            ctx.append(f"- {p['title']} | {p['url']}")

    context = "\n".join(ctx) if ctx else "ï¼ˆæœ¬é€±å¤–éƒ¨è³‡æ–™æŠ“å–å—é™ï¼Œè«‹ä»¥ä½ å° AI ç”¢æ¥­çš„æœ€æ–°çŸ¥è­˜è£œå……ï¼‰"

    # ä¸ŠæœŸæ¨™é¡Œï¼ˆå»é‡ç”¨ï¼‰
    prev_titles = load_prev_titles()
    prev_titles_str = ""
    if prev_titles:
        sample = prev_titles[:20]
        prev_titles_str = "\n\nã€ä¸ŠæœŸå·²å ±å°æ¨™é¡Œï¼ˆè«‹å‹¿é‡è¤‡é€™äº›è©±é¡Œï¼‰ã€‘\n" + "\n".join(f"- {t}" for t in sample)

    prompt = f"""ä½ æ˜¯ä¸€ä½é ‚å°–çš„ AI ç”¢æ¥­åˆ†æå¸«ï¼Œå°ˆé–€ç‚ºç¹é«”ä¸­æ–‡è»Ÿé«”é–‹ç™¼åœ˜éšŠæ’°å¯«æ·±åº¦ AI è¶¨å‹¢é€±å ±ã€‚

ã€è³‡è¨Šä¾†æºæ¯”ä¾‹è¦å‰‡ã€‘
- 80% å…§å®¹ä¾†è‡ªæ­ç¾ä¾†æºï¼ˆRedditã€OpenAI Blogã€Anthropic Blogã€Google DeepMindã€Product Huntã€Tech in Asiaã€CNAï¼‰
- 20% å…§å®¹ä¾†è‡ªä¸­æ–‡ä¾†æºï¼ˆæ©Ÿå™¨ä¹‹å¿ƒã€é‡å­ä½ï¼‰
- å„ªå…ˆé¸å–æ­ç¾å¤§å» å®˜æ–¹éƒ¨è½æ ¼ï¼ˆOpenAI / Anthropic / DeepMindï¼‰çš„ç¬¬ä¸€æ‰‹è³‡è¨Š

ã€è®€è€…èƒŒæ™¯ã€‘
æœ¬å ±å‘Šçš„è®€è€…æ˜¯ä¸€å®¶ 43 äººçš„è»Ÿé«”å…¬å¸ï¼Œå…¶ä¸­ï¼š
- 16 ä½ Web ç”¢å“å·¥ç¨‹å¸«ï¼ˆé—œæ³¨ï¼šAI API æ•´åˆã€å‰ç«¯ AI æ¡†æ¶ã€LLM æ‡‰ç”¨é–‹ç™¼ã€ç”Ÿç”¢åŠ›å·¥å…·ï¼‰
- 6 ä½ C++ ç”¢å“å·¥ç¨‹å¸«ï¼ˆé—œæ³¨ï¼šæ¨è«–æ•ˆèƒ½ã€é‚Šç·£éƒ¨ç½²ã€CUDA/NPU åŠ é€Ÿã€llama.cppã€ONNXï¼‰
- å…¶é¤˜ç‚º PMã€QAã€è¨­è¨ˆã€ç®¡ç†å±¤ï¼ˆé—œæ³¨ï¼šç”¢æ¥­å‹•æ…‹ã€ç«¶å“æ©Ÿæœƒã€AI ç­–ç•¥ï¼‰

æœ¬é€±ç™¼é€æ—¥æœŸï¼š{date}ï¼ˆ{year}å¹´{month}æœˆï¼‰

ä»¥ä¸‹æ˜¯æœ¬é€±å¾å¤šå€‹ä¾†æºæ”¶é›†åˆ°çš„åŸå§‹è³‡æ–™ï¼š

{context}{prev_titles_str}

è«‹æ ¹æ“šä¸Šè¿°è³‡æ–™ï¼Œçµåˆä½ å° AI ç”¢æ¥­çš„æœ€æ–°çŸ¥è­˜ï¼Œç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ä¸€ä»½æ·±åº¦æ¯é€± AI å¿«å ±ã€‚
åš´æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼ˆä½¿ç”¨ Telegram HTML æ ¼å¼ï¼Œå‹¿è¼¸å‡ºä»»ä½•é¡å¤–èªªæ˜æ–‡å­—ï¼‰ï¼š

é€™æ˜¯æœ¬é€±æœ€æ–° AI ç™¼å±•è¶¨å‹¢èˆ‡å·¥å…·æ•´ç†ï¼Œç”±å°ç§˜æ›¸ Bot æ¯é€±ä¸€è‡ªå‹•ç™¼é€ã€‚

ğŸš€ <b>{year}å¹´{month}æœˆï¼šæœ¬é€± AI ç”¢æ¥­æ ¸å¿ƒå‹•æ…‹</b>

â€¢ <b>ã€è¶¨å‹¢ä¸»é¡Œä¸€ã€‘</b>ï¼šä¸€å¥è©±æ‘˜è¦
  â€¢ <b>å…·é«”æ–°èä¸€</b>ï¼š2-3 å¥è©³ç´°èªªæ˜ï¼Œå«å½±éŸ¿ã€äº®é»ã€æ•¸æ“š
  â€¢ <b>å…·é«”æ–°èäºŒ</b>ï¼š2-3 å¥è©³ç´°èªªæ˜
  â€¢ <b>å…·é«”æ–°èä¸‰</b>ï¼š2-3 å¥è©³ç´°èªªæ˜

â€¢ <b>ã€è¶¨å‹¢ä¸»é¡ŒäºŒã€‘</b>ï¼šä¸€å¥è©±æ‘˜è¦
  â€¢ <b>å…·é«”æ–°èä¸€</b>ï¼š2-3 å¥è©³ç´°èªªæ˜
  â€¢ <b>å…·é«”æ–°èäºŒ</b>ï¼š2-3 å¥è©³ç´°èªªæ˜

â€¢ <b>ã€è¶¨å‹¢ä¸»é¡Œä¸‰ã€‘</b>ï¼šä¸€å¥è©±æ‘˜è¦
  â€¢ <b>å…·é«”æ–°èä¸€</b>ï¼š2-3 å¥è©³ç´°èªªæ˜
  â€¢ <b>å…·é«”æ–°èäºŒ</b>ï¼š2-3 å¥è©³ç´°èªªæ˜

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ› ï¸ <b>æœ¬é€±ç ”ç™¼ç²¾é¸å·¥å…·</b>ï¼ˆå‰ç«¯ / API / LLM æ‡‰ç”¨ / æ¨è«–æ•ˆèƒ½ / é‚Šç·£éƒ¨ç½²ï¼‰

â€¢ <b>å·¥å…·æˆ–æ¡†æ¶åç¨±</b> <i>ã€ç«‹å³å¯ç”¨ã€‘</i>
  çˆ†ç´…äº®é»ï¼šèªªæ˜ç‚ºä½•åœ¨ Reddit/PH å¼•çˆ†è¨è«–ï¼Œæ ¸å¿ƒåŠŸèƒ½æ˜¯ä»€éº¼
  å°ç ”ç™¼çš„åƒ¹å€¼ï¼šå…·é«”èªªæ˜èƒ½è§£æ±ºä»€éº¼é–‹ç™¼ç—›é»ã€æ•ˆèƒ½æå‡æˆ–åŠ é€Ÿå“ªå€‹å·¥ä½œæµç¨‹
  ğŸ”— <a href="URL">äº†è§£æ›´å¤š</a>

â€¢ <b>å·¥å…·æˆ–æ¡†æ¶åç¨±</b> <i>ã€ç«‹å³å¯ç”¨ã€‘</i>
  çˆ†ç´…äº®é»ï¼šèªªæ˜
  å°ç ”ç™¼çš„åƒ¹å€¼ï¼šèªªæ˜
  ğŸ”— <a href="URL">äº†è§£æ›´å¤š</a>

â€¢ <b>å·¥å…·æˆ–æ¡†æ¶åç¨±</b> <i>ã€å€¼å¾—è©•ä¼°ã€‘</i>
  çˆ†ç´…äº®é»ï¼šèªªæ˜
  å°ç ”ç™¼çš„åƒ¹å€¼ï¼šèªªæ˜
  ğŸ”— <a href="URL">äº†è§£æ›´å¤š</a>

â€¢ <b>å·¥å…·æˆ–æ¡†æ¶åç¨±</b> <i>ã€å€¼å¾—è©•ä¼°ã€‘</i>
  çˆ†ç´…äº®é»ï¼šèªªæ˜
  å°ç ”ç™¼çš„åƒ¹å€¼ï¼šèªªæ˜
  ğŸ”— <a href="URL">äº†è§£æ›´å¤š</a>

â€¢ <b>å·¥å…·æˆ–æ¡†æ¶åç¨±</b> <i>ã€æŒçºŒè§€å¯Ÿã€‘</i>
  çˆ†ç´…äº®é»ï¼šèªªæ˜
  å°ç ”ç™¼çš„åƒ¹å€¼ï¼šèªªæ˜
  ğŸ”— <a href="URL">äº†è§£æ›´å¤š</a>

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ’¡ <b>æ·±åº¦è§€å¯Ÿï¼šå°æˆ‘å€‘åœ˜éšŠçš„å½±éŸ¿</b>

1. <b>è§€å¯Ÿä¸»é¡Œä¸€</b>ï¼š
3-4 å¥æ·±åº¦åˆ†æï¼Œèªªæ˜æ­¤è¶¨å‹¢å° Web æˆ– C++ ç”¢å“åœ˜éšŠçš„å¯¦éš›å½±éŸ¿èˆ‡å…·é«”æ‡‰å°å»ºè­°ã€‚

2. <b>è§€å¯Ÿä¸»é¡ŒäºŒ</b>ï¼š
3-4 å¥æ·±åº¦åˆ†æï¼Œèšç„¦ç«¶å“æ©Ÿæœƒæˆ–åœ˜éšŠå¯æ¡å–çš„è¡Œå‹•ã€‚

3. <b>è§€å¯Ÿä¸»é¡Œä¸‰</b>ï¼š
3-4 å¥æ·±åº¦åˆ†æï¼Œæå‡ºç¨åˆ°æ´è¦‹è€Œéåªæ˜¯æ‘˜è¦æ–°èã€‚

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â° é€±å ±ç™¼é€æ™‚é–“ï¼š{date} {now.strftime('%H:%M')}ï¼ˆæ¯é€±ä¸€ï¼‰

è¼¸å‡ºè¦å‰‡ï¼ˆå¿…é ˆéµå®ˆï¼‰ï¼š
1. åƒ…ä½¿ç”¨ Telegram æ”¯æ´çš„ HTML æ¨™ç±¤ï¼š<b> <i> <a href="..."> <code> <pre>
2. ä¸€èˆ¬æ–‡å­—ä¸­è‹¥å‡ºç¾ & < > å¿…é ˆè½‰ç¾©ç‚º &amp; &lt; &gt;
3. URL å¿…é ˆä½¿ç”¨æœ¬é€±è³‡æ–™ä¸­çœŸå¯¦å­˜åœ¨çš„é€£çµï¼›è‹¥ç„¡åˆé©é€£çµï¼Œä½¿ç”¨è©²å·¥å…·å®˜ç¶²
4. ã€ç«‹å³å¯ç”¨ã€‘= ä»Šå¤©å°±èƒ½è©¦ç”¨ï¼›ã€å€¼å¾—è©•ä¼°ã€‘= éœ€ 1 é€±è©•ä¼°ï¼›ã€æŒçºŒè§€å¯Ÿã€‘= å°šåœ¨æ—©æœŸ
5. ç ”ç™¼ç²¾é¸å·¥å…·æ¶µè“‹ Webï¼ˆå‰ç«¯ã€LLM APIï¼‰èˆ‡ C++ï¼ˆæ¨è«–ã€é‚Šç·£éƒ¨ç½²ï¼‰å…©é¡ï¼Œåˆä½µå‘ˆç¾ï¼Œä¸åˆ†åœ˜éšŠæ¨™ç±¤ï¼Œé¸ 4ï½5 å€‹æœ€å…·ä»£è¡¨æ€§å·¥å…·
6. æ·±åº¦è§€å¯Ÿè¦èšç„¦è»Ÿé«”é–‹ç™¼å…¬å¸è¦–è§’ï¼Œæœ‰å…·é«”è¡Œå‹•å»ºè­°
7. å…¨æ–‡ç¹é«”ä¸­æ–‡
8. è‹¥æœ‰ã€ä¸ŠæœŸå·²å ±å°æ¨™é¡Œã€‘æ¸…å–®ï¼Œè©²æ¸…å–®ä¸­å‡ºç¾çš„ç›¸åŒè©±é¡Œæˆ–æ–°èäº‹ä»¶æœ¬æœŸä¸€å¾‹è·³éï¼Œæ”¹é¸å…¶ä»–æ–°å…§å®¹
"""

    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
    msg = client.messages.create(
        model=CLAUDE_MODEL,
        max_tokens=3500,
        messages=[{"role": "user", "content": prompt}],
    )
    return msg.content[0].text.strip()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å ±å‘Šå¿«å–ï¼šç”Ÿæˆå¾Œå­˜æª”ï¼Œ/preview å„ªå…ˆè®€å–ä¸é‡å‘¼ API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_report_cache(report: str):
    """å°‡æœ¬æœŸå ±å‘Šå­˜å…¥å¿«å–æª”"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    cache = {
        "generated_at": datetime.now().isoformat(),
        "report":        report,
    }
    with open(REPORT_CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)
    print(f"  ğŸ’¾ å ±å‘Šå·²å­˜å…¥å¿«å–ï¼ˆ{REPORT_CACHE_FILE}ï¼‰", flush=True)


def load_report_cache() -> str | None:
    """è®€å–å¿«å–å ±å‘Šï¼Œå›å‚³å ±å‘Šæ–‡å­—ï¼›ç„¡å¿«å–æ™‚å›å‚³ None"""
    if not REPORT_CACHE_FILE.exists():
        return None
    try:
        with open(REPORT_CACHE_FILE, encoding="utf-8") as f:
            data = json.load(f)
        return data.get("report")
    except Exception:
        return None


def get_cache_info() -> dict | None:
    """è®€å–å¿«å–å…ƒè³‡è¨Šï¼ˆç”Ÿæˆæ™‚é–“ç­‰ï¼‰ï¼Œç„¡å¿«å–æ™‚å›å‚³ None"""
    if not REPORT_CACHE_FILE.exists():
        return None
    try:
        with open(REPORT_CACHE_FILE, encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Telegram ç™¼é€ï¼ˆæ”¯æ´å¤šå€‹ Chat IDï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _split_chunks(text, max_len=4000):
    """å°‡é•·æ–‡å­—åˆ‡åˆ†æˆä¸è¶…é max_len çš„ç‰‡æ®µ"""
    if len(text) <= max_len:
        return [text]
    lines, chunks, buf = text.split("\n"), [], ""
    for line in lines:
        if len(buf) + len(line) + 1 > max_len:
            chunks.append(buf)
            buf = line
        else:
            buf = (buf + "\n" + line) if buf else line
    if buf:
        chunks.append(buf)
    return chunks


def _send_one_chunk(chat_id, text):
    """å‘å–®ä¸€ Chat ID ç™¼é€ä¸€å‰‡è¨Šæ¯"""
    url  = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    data = json.dumps({
        "chat_id":                  chat_id,
        "text":                     text,
        "parse_mode":               "HTML",
        "disable_web_page_preview": True,
    }).encode("utf-8")
    req = urllib.request.Request(url, data=data, method="POST")
    req.add_header("Content-Type", "application/json; charset=utf-8")
    try:
        with urllib.request.urlopen(req, timeout=TIMEOUT) as r:
            return json.loads(r.read().decode())
    except urllib.error.HTTPError as e:
        body = e.read().decode(errors="replace")
        print(f"  âŒ HTTP {e.code}: {body}", file=sys.stderr)
        return {"ok": False}
    except Exception as e:
        print(f"  âŒ {e}", file=sys.stderr)
        return {"ok": False}


def send_telegram(text, target_ids=None, max_len=4000):
    """å‘æŒ‡å®š target_ids ç™¼é€è¨Šæ¯ï¼›æœªæŒ‡å®šæ™‚ä½¿ç”¨ç’°å¢ƒè®Šæ•¸çš„ CHAT_IDS"""
    ids     = target_ids if target_ids is not None else CHAT_IDS
    chunks  = _split_chunks(text, max_len)
    results = []

    for chat_id in ids:
        print(f"  ğŸ“¨ ç™¼é€è‡³ Chat ID: {chat_id}")
        for i, chunk in enumerate(chunks, 1):
            res = _send_one_chunk(chat_id, chunk)
            results.append(res)
            ok  = res.get("ok")
            mid = res.get("result", {}).get("message_id", "?")
            print(f"    {'âœ…' if ok else 'âŒ'} ç¬¬ {i}/{len(chunks)} å‰‡ï¼ˆmsg_id={mid}ï¼‰")

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»ç¨‹å¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main(override_chat_ids=None):
    """
    override_chat_ids: ç”±å¤–éƒ¨ï¼ˆbot.pyï¼‰å‚³å…¥çš„æ”¶ä»¶äººæ¸…å–®ã€‚
                       None æ™‚ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ TELEGRAM_CHAT_IDã€‚
    """
    test_mode = "--test" in sys.argv
    target    = override_chat_ids if override_chat_ids is not None else CHAT_IDS

    print(f"\n{'='*54}")
    print(f"  ğŸ¤– æ¯é€± AI å¿«å ± v2 {'ã€æ¸¬è©¦æ¨¡å¼ã€‘' if test_mode else ''}")
    print(f"  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*54}\n")

    # æª¢æŸ¥å¿…è¦è¨­å®š
    missing = [k for k, v in [
        ("ANTHROPIC_API_KEY", ANTHROPIC_API_KEY),
        ("TELEGRAM_BOT_TOKEN", BOT_TOKEN),
    ] if not v]
    if override_chat_ids is None and not _raw_ids:
        missing.append("TELEGRAM_CHAT_ID")
    if missing:
        print(f"âŒ ç¼ºå°‘å¿…è¦è¨­å®šï¼š{', '.join(missing)}")
        print("   è«‹åœ¨åŒç›®éŒ„å»ºç«‹ .env æª”ï¼ˆåƒè€ƒ .env.exampleï¼‰")
        sys.exit(1)
    print(f"  ğŸ“‹ ç™¼é€å°è±¡ï¼š{len(target)} å€‹ Chat IDï¼ˆ{', '.join(target)}ï¼‰")

    # â”€â”€ è®€å–ä¸ŠæœŸæ¨™é¡Œï¼Œä¾›å»é‡ä½¿ç”¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    prev_titles = load_prev_titles()
    if prev_titles:
        print(f"  ğŸ” è¼‰å…¥ä¸ŠæœŸæ¨™é¡Œ {len(prev_titles)} å‰‡ï¼Œå°‡éæ¿¾é‡è¤‡æ–°è", flush=True)

    def dedup(items):
        """éæ¿¾èˆ‡ä¸ŠæœŸæ¨™é¡Œé‡è¤‡çš„æ¢ç›®"""
        return [i for i in items if not _is_duplicate(i["title"], prev_titles)]

    # â”€â”€ æ”¶é›†åŸå§‹è³‡æ–™ï¼ˆ80% æ­ç¾ï¼Œ20% ä¸­æ–‡ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ğŸ“¡ æ”¶é›†å„ä¾†æºè³‡æ–™ä¸­...")
    raw = {}

    # â”€â”€ æ­ç¾ä¾†æºï¼ˆ80%ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("  â†’ Redditï¼ˆr/artificial / MachineLearning / LocalLLaMA / ChatGPT / singularityï¼‰")
    raw["reddit"] = dedup(fetch_reddit(top_n=10))

    print("  â†’ Product Huntï¼ˆAI å·¥å…·ç¯©é¸ï¼‰")
    raw["producthunt"] = dedup(fetch_rss(
        "https://www.producthunt.com/feed", "Product Hunt", max_items=6, ai_filter=True))

    print("  â†’ OpenAI Blog")
    raw["openai"] = dedup(fetch_rss(
        "https://openai.com/blog/rss.xml", "OpenAI Blog", max_items=4, ai_filter=False))

    print("  â†’ Anthropic Blog")
    raw["anthropic"] = dedup(fetch_rss(
        "https://www.anthropic.com/rss.xml", "Anthropic Blog", max_items=4, ai_filter=False))

    print("  â†’ Google DeepMind Blog")
    raw["deepmind"] = dedup(fetch_rss(
        "https://deepmind.google/blog/rss.xml", "Google DeepMind", max_items=4, ai_filter=False))

    print("  â†’ Tech in Asiaï¼ˆAI ç¯©é¸ï¼‰")
    raw["techinasia"] = dedup(fetch_rss(
        "https://www.techinasia.com/feed", "Tech in Asia", max_items=4, ai_filter=True))

    print("  â†’ CNA Technology News")
    raw["cna"] = dedup(fetch_rss(
        "https://www.channelnewsasia.com/api/v1/rss-outbound-feed?_format=xml&category=10416",
        "CNA Tech", max_items=4, ai_filter=True))

    # â”€â”€ ä¸­æ–‡ä¾†æºï¼ˆ20%ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("  â†’ æ©Ÿå™¨ä¹‹å¿ƒ jiqizhixin.com")
    raw["jiqizhixin"] = dedup(fetch_rss(
        "https://www.jiqizhixin.com/rss", "æ©Ÿå™¨ä¹‹å¿ƒ", max_items=4))

    print("  â†’ é‡å­ä½ qbitai.com")
    raw["qbitai"] = dedup(fetch_rss(
        "https://www.qbitai.com/feed", "é‡å­ä½", max_items=3))

    total = sum(len(v) for v in raw.values())
    print(f"\n  ğŸ“Š å…±æ”¶é›† {total} å‰‡åŸå§‹è³‡æ–™ï¼ˆå»é‡å¾Œï¼‰\n")

    # å‘¼å« Claude ç”Ÿæˆæ·±åº¦å ±å‘Š
    print("ğŸ§  Claude ç”Ÿæˆæ·±åº¦åˆ†æå ±å‘Šä¸­ï¼ˆç´„ 10ï½20 ç§’ï¼‰...")
    report = generate_report(raw)
    print(f"  âœ… å ±å‘Šç”Ÿæˆå®Œæˆï¼ˆ{len(report)} å­—å…ƒï¼‰\n")

    # å­˜å…¥å¿«å–ï¼Œä¾›å¾ŒçºŒ /preview ç›´æ¥è®€å–
    save_report_cache(report)

    # å„²å­˜æœ¬æœŸæ‰€æœ‰æ”¶é›†åˆ°çš„æ¨™é¡Œï¼Œä¾›ä¸‹æœŸå»é‡ä½¿ç”¨
    all_titles = [item["title"] for items in raw.values() for item in items]
    save_prev_titles(all_titles)
    print(f"  ğŸ“ å·²å„²å­˜ {len(all_titles)} å‰‡æ¨™é¡Œä¾›ä¸‹æœŸå»é‡ä½¿ç”¨", flush=True)

    # ç™¼é€ Emailï¼ˆè‹¥å·²è¨­å®š EMAIL_HOST ç­‰ç’°å¢ƒè®Šæ•¸ï¼‰
    try:
        import email_sender
        if email_sender.is_configured():
            email_sender.send_weekly_report(report)
    except Exception as e:
        print(f"  âš ï¸ Email æ¨¡çµ„ç™¼ç”ŸéŒ¯èª¤ï¼š{e}", flush=True)

    if test_mode:
        print("â”€â”€â”€ å ±å‘Šé è¦½ " + "â”€" * 40)
        print(report)
        print("â”€" * 54)
        print("\nâœ… æ¸¬è©¦å®Œæˆï¼ˆæœªç™¼é€è‡³ Telegramï¼‰")
        return

    print("ğŸ“¤ ç™¼é€è‡³ Telegram...")
    results = send_telegram(report, target_ids=target)
    ok = sum(1 for r in results if r.get("ok"))
    print(f"\n{'âœ…' if ok == len(results) else 'âš ï¸'} å®Œæˆï¼æˆåŠŸ {ok}/{len(results)} å‰‡")
    if ok < len(results):
        sys.exit(1)


if __name__ == "__main__":
    main()
